{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "_CMP = '_cmp'\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def open_db_connection(*, file, close=True,\n",
    "                       lock=None, check_same_thread=False):\n",
    "    \"\"\"\n",
    "    Safety wrapper for the database call.\n",
    "    \"\"\"\n",
    "\n",
    "    if lock is not None:\n",
    "        lock.acquire()\n",
    "\n",
    "    con = sql.connect(database=file, check_same_thread=check_same_thread)\n",
    "\n",
    "    try:\n",
    "        yield con\n",
    "\n",
    "    finally:\n",
    "        if close:\n",
    "            con.close()\n",
    "        if lock is not None:\n",
    "            lock.release()\n",
    "\n",
    "\n",
    "def get_table_name(file):\n",
    "    with open_db_connection(file=file, close=True) as con:\n",
    "        res = pd.read_sql_query(sql=\"SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%'\",\n",
    "                                con=con)\n",
    "        return res['name'].values\n",
    "\n",
    "\n",
    "def rename_table(file, tables):\n",
    "    old_names = get_table_name(file=file)\n",
    "\n",
    "    with open_db_connection(file=file, close=True) as con:\n",
    "        cur = con.cursor()\n",
    "        for old in old_names:\n",
    "            if old in tables:\n",
    "                new = tables['old']\n",
    "                cur.execute(f\"ALTER TABLE `{old}` RENAME TO `{new}`\")\n",
    "\n",
    "\n",
    "def get_values_sql(*, file, table='db', columns=None, rows=-1,\n",
    "                   values_only=False, squeeze_col=True, squeeze_row=True):\n",
    "    \"\"\"\n",
    "    'i_samples' == i_samples_global\n",
    "    \"\"\"\n",
    "\n",
    "    lock = None  # Lock is not necessary fo reading\n",
    "    if columns is None:\n",
    "        columns = '*'\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    columns_str = ', '.join(map(str, columns))\n",
    "\n",
    "    if isinstance(rows, int):\n",
    "        rows = [rows]\n",
    "    rows = np.array(rows)\n",
    "\n",
    "    if rows[0] == -1:  # All samples\n",
    "        with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "            df = pd.read_sql_query(con=con, sql=f\"SELECT {columns_str} FROM {table}\")  # path_db\n",
    "\n",
    "    else:\n",
    "        rows_str = rows + 1  # Attention! Unlike in Python, SQL indices start at 1\n",
    "        rows_str = ', '.join(map(str, rows_str))\n",
    "        with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "            df = pd.read_sql_query(sql=f\"SELECT {columns_str} FROM {table} WHERE ROWID in ({rows_str})\",\n",
    "                                   index_col=rows, con=con)\n",
    "\n",
    "    value_list = []\n",
    "    if np.any(columns == ['*']):\n",
    "        columns = df.columns.values\n",
    "\n",
    "    if values_only:\n",
    "        for col in columns:\n",
    "            value = __decompress_values(value=df.loc[:, col].values, col=col)\n",
    "            value_list.append(value)\n",
    "\n",
    "        if len(df) == 1 and squeeze_row:\n",
    "            for i in range(len(columns)):\n",
    "                value_list[i] = value_list[i][0]\n",
    "\n",
    "        if len(value_list) == 1 and squeeze_col:\n",
    "            value_list = value_list[0]\n",
    "\n",
    "        return value_list\n",
    "\n",
    "    # Return pandas.DataFrame\n",
    "    else:\n",
    "        for col in columns:\n",
    "            value = __decompress_values(value=df.loc[:, col].values, col=col)\n",
    "            df.loc[:, col] = numeric2object_array(value)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def set_values_sql(*, file, table='db',\n",
    "                   values, columns, rows=-1, lock=None):\n",
    "    \"\"\"\n",
    "    Note: multidimensional numpy arrays have to be saved as flat to SQL otherwise the order is messed up\n",
    "    values = ([...], [...], [...], ...)\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle rows argument\n",
    "    if isinstance(rows, int):\n",
    "        if rows == -1:\n",
    "            rows = np.arange(len(values[0])).tolist()\n",
    "        else:\n",
    "            rows = [rows]\n",
    "\n",
    "    rows_sql = (np.array(rows) + 1).tolist()  # Attention! Unlike in Python, SQL indices start at 1\n",
    "\n",
    "    # Handle columns argument\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    columns_str = '=?, '.join(map(str, columns))\n",
    "    columns_str += '=?'\n",
    "\n",
    "    values_rows_sql = change_tuple_order(values + (rows_sql,))\n",
    "    values_rows_sql = list(values_rows_sql)\n",
    "    query = f\"UPDATE {table} SET {columns_str} WHERE ROWID=?\"\n",
    "\n",
    "    with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "        cur = con.cursor()\n",
    "        if len(values_rows_sql) == 1:\n",
    "            cur.execute(query, values_rows_sql[0])\n",
    "        else:\n",
    "            cur.executemany(query, values_rows_sql)\n",
    "\n",
    "        con.commit()\n",
    "\n",
    "\n",
    "def df2sql(df, file, table='db', if_exists='fail', lock=None):\n",
    "    \"\"\"\n",
    "    From DataFrame.to_sql():\n",
    "        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n",
    "                   - fail: If table exists, do nothing.\n",
    "                   - replace: If table exists, drop it, recreate it, and insert Measurements.\n",
    "                   - append: If table exists, insert Measurements. Create if does not exist.\n",
    "    \"\"\"\n",
    "    with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "        df.to_sql(name=table, con=con, if_exists=if_exists, index=False)\n",
    "\n",
    "\n",
    "# Helper\n",
    "# Image Compression <-> Decompression\n",
    "def __decompress_values(value, col):\n",
    "    # SQL saves everything in binary form -> convert back to numeric, expect the columns which are marked as CMP\n",
    "    if isinstance(value[0], bytes) and col[-4:] != _CMP:\n",
    "        if col in ['i_world', 'i_sample', 'n_obstacles']:\n",
    "            value = np.array([np.frombuffer(v, dtype=int) for v in value], dtype=int)\n",
    "        elif col in ['rectangle_pos', 'rectangle_position', 'rectangle_size']:\n",
    "            value = np.array([np.frombuffer(v, dtype=int) for v in value], dtype=object)\n",
    "        else:\n",
    "            value = np.array([np.frombuffer(v, dtype=float) for v in value])\n",
    "        value = np.squeeze(value)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def change_tuple_order(tpl):\n",
    "    return tuple(map(lambda *tt: tuple(tt), *tpl))\n",
    "\n",
    "\n",
    "def numeric2object_array(arr):\n",
    "    n = arr.shape[0]\n",
    "    arr_obj = np.zeros(n, dtype=object)\n",
    "    for i in range(n):\n",
    "        arr_obj[i] = arr[i]\n",
    "\n",
    "    return arr_obj\n",
    "\n",
    "\n",
    "def object2numeric_array(arr):\n",
    "    s = np.shape(arr)\n",
    "    arr = np.array([v for v in np.ravel(arr)])\n",
    "    arr = np.reshape(arr, s + np.shape(arr)[1:])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def initialize_array(shape, mode='zeros', dtype=None, order='c'):\n",
    "\n",
    "    if mode == 'zeros':\n",
    "        return np.zeros(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'ones':\n",
    "        return np.ones(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'empty':\n",
    "        return np.empty(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'random':\n",
    "        return np.random.random(shape).astype(dtype=dtype, order=order)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown initialization method {mode}\")\n",
    "\n",
    "\n",
    "def __dim_voxels(n_voxels, n_dim=None):\n",
    "    if np.size(n_voxels) == 1:\n",
    "        try:\n",
    "            n_voxels = tuple(n_voxels)\n",
    "        except TypeError:\n",
    "            n_voxels = (n_voxels,)\n",
    "        n_voxels *= n_dim\n",
    "    else:\n",
    "        n_voxels = tuple(n_voxels)\n",
    "\n",
    "    return n_voxels\n",
    "\n",
    "\n",
    "def image_array_shape(n_voxels, n_samples=None, n_dim=None, n_channels=None):\n",
    "    \"\"\"\n",
    "    Helper to set the shape for an image array.\n",
    "    n_samples=100,  n_voxels=64,          n_dim=2,    n_channels=None  ->  (100, 64, 64)\n",
    "    n_samples=100,  n_voxels=64,          n_dim=3,    n_channels=2     ->  (100, 64, 64, 64, 2)\n",
    "    n_samples=None, n_voxel=(10, 11, 12), n_dim=None, n_channels=None  ->  (10, 11, 12)\n",
    "    \"\"\"\n",
    "\n",
    "    shape = __dim_voxels(n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "    if n_samples is not None:\n",
    "        shape = (n_samples,) + shape\n",
    "    if n_channels is not None:\n",
    "        shape = shape + (n_channels,)\n",
    "\n",
    "    return shape\n",
    "\n",
    "\n",
    "def initialize_image_array(n_voxels, n_dim=None, n_samples=None, n_channels=None,\n",
    "                           dtype=bool, initialization='zeros'):\n",
    "    shape = image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_samples=n_samples, n_channels=n_channels)\n",
    "    return initialize_array(shape=shape, mode=initialization, dtype=dtype)\n",
    "\n",
    "\n",
    "# Image Compression <-> Decompression\n",
    "def img2compressed(img, n_dim=-1, level=9):\n",
    "    \"\"\"\n",
    "    Compress the given image with the zlib routine to a binary string.\n",
    "    Level of compression can be adjusted. A timing with respect to different compression levels for decompression showed\n",
    "    no difference, so the highest level is default, this corresponds to the largest compression.\n",
    "    For compression it is slightly slower but this happens just once and not during keras training, so the smaller\n",
    "    needed memory was favoured.\n",
    "    Alternative:\n",
    "    <-> use numpy sparse for the world images, especially in 3d  -> zlib is more effective and more general\n",
    "    \"\"\"\n",
    "\n",
    "    if n_dim == -1:\n",
    "        return zlib.compress(img.tobytes(), level=level)\n",
    "    else:\n",
    "        shape = img.shape[:-n_dim]\n",
    "        img_cmp = np.empty(shape, dtype=object)\n",
    "        for idx in np.ndindex(*shape):\n",
    "            img_cmp[idx] = zlib.compress(img[idx, ...].tobytes(), level=level)\n",
    "        return img_cmp\n",
    "\n",
    "\n",
    "def compressed2img(img_cmp, n_voxels, n_dim=None, n_channels=None, dtype=bool):\n",
    "    \"\"\"\n",
    "    Decompress the binary string back to an image of given shape\n",
    "    \"\"\"\n",
    "\n",
    "    shape = np.shape(img_cmp)\n",
    "\n",
    "    if shape:\n",
    "        n_samples = np.size(img_cmp)\n",
    "        img_arr = initialize_image_array(n_voxels=n_voxels, n_dim=n_dim, n_samples=n_samples, n_channels=n_channels,\n",
    "                                         dtype=dtype)\n",
    "        for i in range(n_samples):\n",
    "            img_arr[i, ...] = np.fromstring(zlib.decompress(img_cmp[i]), dtype=dtype).reshape(\n",
    "                image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_channels=n_channels))\n",
    "        return img_arr\n",
    "\n",
    "    else:\n",
    "        return np.fromstring(zlib.decompress(img_cmp), dtype=dtype).reshape(\n",
    "            image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_channels=n_channels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO change to you own directory\n",
    "file = './SingleSphere02.db'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_voxels = 64\n",
    "voxel_size = 10 / 64     # in m\n",
    "extent = [0, 10, 0, 10]  # in m\n",
    "n_waypoints = 22  # start + 20 inner points + end\n",
    "n_dim = 2\n",
    "n_paths_per_world = 1000\n",
    "n_worlds = 5000\n",
    "\n",
    "\n",
    "worlds = get_values_sql(file=file, table='worlds')\n",
    "obstacle_images = compressed2img(img_cmp=worlds.obst_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "print(obstacle_images[0].shape)\n",
    "# always 1000 paths belong to one world\n",
    "# 0...999     -> world 0\n",
    "# 1000...1999 -> world 1\n",
    "# 2000...2999 -> world 2\n",
    "paths = get_values_sql(file=file, table='paths', rows=[0, 1, 2, 1000, 2000])\n",
    "path_images = compressed2img(img_cmp=paths.path_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "start_images = compressed2img(img_cmp=paths.start_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "end_images = compressed2img(img_cmp=paths.end_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "q_paths = object2numeric_array(paths.q_path.values)\n",
    "q_paths = q_paths.reshape(-1, n_waypoints, n_dim)\n",
    "\n",
    "# Plot an example\n",
    "i = 0\n",
    "i_world = paths.i_world[i]\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "#ax.imshow(obstacle_images[0].T)\n",
    "print(i_world)\n",
    "ax.imshow(obstacle_images[i].T, origin='lower', extent=extent, cmap='binary',)\n",
    "ax.imshow(start_images[i].T, origin='lower', extent=extent, cmap='Greens', alpha=0.4)\n",
    "ax.imshow(end_images[i].T, origin='lower', extent=extent, cmap='Reds', alpha=0.4)\n",
    "ax.imshow(path_images[i].T, origin='lower', extent=extent, cmap='Blues', alpha=0.2)\n",
    "ax.axis('off')\n",
    "ax.plot(*q_paths[i].T, color='k', marker='o')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
