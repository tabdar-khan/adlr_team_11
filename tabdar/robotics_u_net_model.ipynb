{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdae782",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b3e1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 01:09:11.087492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-13 01:09:11.087561: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730bd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "from contextlib import contextmanager\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "_CMP = '_cmp'\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def open_db_connection(*, file, close=True,\n",
    "                       lock=None, check_same_thread=False):\n",
    "    \"\"\"\n",
    "    Safety wrapper for the database call.\n",
    "    \"\"\"\n",
    "\n",
    "    if lock is not None:\n",
    "        lock.acquire()\n",
    "\n",
    "    con = sql.connect(database=file, check_same_thread=check_same_thread)\n",
    "\n",
    "    try:\n",
    "        yield con\n",
    "\n",
    "    finally:\n",
    "        if close:\n",
    "            con.close()\n",
    "        if lock is not None:\n",
    "            lock.release()\n",
    "\n",
    "\n",
    "def get_table_name(file):\n",
    "    with open_db_connection(file=file, close=True) as con:\n",
    "        res = pd.read_sql_query(sql=\"SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%'\",\n",
    "                                con=con)\n",
    "        return res['name'].values\n",
    "\n",
    "\n",
    "def rename_table(file, tables):\n",
    "    old_names = get_table_name(file=file)\n",
    "\n",
    "    with open_db_connection(file=file, close=True) as con:\n",
    "        cur = con.cursor()\n",
    "        for old in old_names:\n",
    "            if old in tables:\n",
    "                new = tables['old']\n",
    "                cur.execute(f\"ALTER TABLE `{old}` RENAME TO `{new}`\")\n",
    "\n",
    "\n",
    "def get_values_sql(*, file, table='db', columns=None, rows=-1,\n",
    "                   values_only=False, squeeze_col=True, squeeze_row=True):\n",
    "    \"\"\"\n",
    "    'i_samples' == i_samples_global\n",
    "    \"\"\"\n",
    "\n",
    "    lock = None  # Lock is not necessary fo reading\n",
    "    if columns is None:\n",
    "        columns = '*'\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    columns_str = ', '.join(map(str, columns))\n",
    "\n",
    "    if isinstance(rows, int):\n",
    "        rows = [rows]\n",
    "    rows = np.array(rows)\n",
    "\n",
    "    if rows[0] == -1:  # All samples\n",
    "        with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "            df = pd.read_sql_query(con=con, sql=f\"SELECT {columns_str} FROM {table}\")  # path_db\n",
    "\n",
    "    else:\n",
    "        rows_str = rows + 1  # Attention! Unlike in Python, SQL indices start at 1\n",
    "        rows_str = ', '.join(map(str, rows_str))\n",
    "        with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "            df = pd.read_sql_query(sql=f\"SELECT {columns_str} FROM {table} WHERE ROWID in ({rows_str})\",\n",
    "                                   index_col=rows, con=con)\n",
    "\n",
    "    value_list = []\n",
    "    if np.any(columns == ['*']):\n",
    "        columns = df.columns.values\n",
    "\n",
    "    if values_only:\n",
    "        for col in columns:\n",
    "            value = __decompress_values(value=df.loc[:, col].values, col=col)\n",
    "            value_list.append(value)\n",
    "\n",
    "        if len(df) == 1 and squeeze_row:\n",
    "            for i in range(len(columns)):\n",
    "                value_list[i] = value_list[i][0]\n",
    "\n",
    "        if len(value_list) == 1 and squeeze_col:\n",
    "            value_list = value_list[0]\n",
    "\n",
    "        return value_list\n",
    "\n",
    "    # Return pandas.DataFrame\n",
    "    else:\n",
    "        for col in columns:\n",
    "            value = __decompress_values(value=df.loc[:, col].values, col=col)\n",
    "            df.loc[:, col] = numeric2object_array(value)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def set_values_sql(*, file, table='db',\n",
    "                   values, columns, rows=-1, lock=None):\n",
    "    \"\"\"\n",
    "    Note: multidimensional numpy arrays have to be saved as flat to SQL otherwise the order is messed up\n",
    "    values = ([...], [...], [...], ...)\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle rows argument\n",
    "    if isinstance(rows, int):\n",
    "        if rows == -1:\n",
    "            rows = np.arange(len(values[0])).tolist()\n",
    "        else:\n",
    "            rows = [rows]\n",
    "\n",
    "    rows_sql = (np.array(rows) + 1).tolist()  # Attention! Unlike in Python, SQL indices start at 1\n",
    "\n",
    "    # Handle columns argument\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    columns_str = '=?, '.join(map(str, columns))\n",
    "    columns_str += '=?'\n",
    "\n",
    "    values_rows_sql = change_tuple_order(values + (rows_sql,))\n",
    "    values_rows_sql = list(values_rows_sql)\n",
    "    query = f\"UPDATE {table} SET {columns_str} WHERE ROWID=?\"\n",
    "\n",
    "    with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "        cur = con.cursor()\n",
    "        if len(values_rows_sql) == 1:\n",
    "            cur.execute(query, values_rows_sql[0])\n",
    "        else:\n",
    "            cur.executemany(query, values_rows_sql)\n",
    "\n",
    "        con.commit()\n",
    "\n",
    "\n",
    "def df2sql(df, file, table='db', if_exists='fail', lock=None):\n",
    "    \"\"\"\n",
    "    From DataFrame.to_sql():\n",
    "        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n",
    "                   - fail: If table exists, do nothing.\n",
    "                   - replace: If table exists, drop it, recreate it, and insert Measurements.\n",
    "                   - append: If table exists, insert Measurements. Create if does not exist.\n",
    "    \"\"\"\n",
    "    with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "        df.to_sql(name=table, con=con, if_exists=if_exists, index=False)\n",
    "\n",
    "\n",
    "# Helper\n",
    "# Image Compression <-> Decompression\n",
    "def __decompress_values(value, col):\n",
    "    # SQL saves everything in binary form -> convert back to numeric, expect the columns which are marked as CMP\n",
    "    if isinstance(value[0], bytes) and col[-4:] != _CMP:\n",
    "        if col in ['i_world', 'i_sample', 'n_obstacles']:\n",
    "            value = np.array([np.frombuffer(v, dtype=int) for v in value], dtype=int)\n",
    "        elif col in ['rectangle_pos', 'rectangle_position', 'rectangle_size']:\n",
    "            value = np.array([np.frombuffer(v, dtype=int) for v in value], dtype=object)\n",
    "        else:\n",
    "            value = np.array([np.frombuffer(v, dtype=float) for v in value])\n",
    "        value = np.squeeze(value)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def change_tuple_order(tpl):\n",
    "    return tuple(map(lambda *tt: tuple(tt), *tpl))\n",
    "\n",
    "\n",
    "def numeric2object_array(arr):\n",
    "    n = arr.shape[0]\n",
    "    arr_obj = np.zeros(n, dtype=object)\n",
    "    for i in range(n):\n",
    "        arr_obj[i] = arr[i]\n",
    "\n",
    "    return arr_obj\n",
    "\n",
    "\n",
    "def object2numeric_array(arr):\n",
    "    s = np.shape(arr)\n",
    "    arr = np.array([v for v in np.ravel(arr)])\n",
    "    arr = np.reshape(arr, s + np.shape(arr)[1:])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def initialize_array(shape, mode='zeros', dtype=None, order='c'):\n",
    "\n",
    "    if mode == 'zeros':\n",
    "        return np.zeros(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'ones':\n",
    "        return np.ones(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'empty':\n",
    "        return np.empty(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'random':\n",
    "        return np.random.random(shape).astype(dtype=dtype, order=order)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown initialization method {mode}\")\n",
    "\n",
    "\n",
    "def __dim_voxels(n_voxels, n_dim=None):\n",
    "    if np.size(n_voxels) == 1:\n",
    "        try:\n",
    "            n_voxels = tuple(n_voxels)\n",
    "        except TypeError:\n",
    "            n_voxels = (n_voxels,)\n",
    "        n_voxels *= n_dim\n",
    "    else:\n",
    "        n_voxels = tuple(n_voxels)\n",
    "\n",
    "    return n_voxels\n",
    "\n",
    "\n",
    "def image_array_shape(n_voxels, n_samples=None, n_dim=None, n_channels=None):\n",
    "    \"\"\"\n",
    "    Helper to set the shape for an image array.\n",
    "    n_samples=100,  n_voxels=64,          n_dim=2,    n_channels=None  ->  (100, 64, 64)\n",
    "    n_samples=100,  n_voxels=64,          n_dim=3,    n_channels=2     ->  (100, 64, 64, 64, 2)\n",
    "    n_samples=None, n_voxel=(10, 11, 12), n_dim=None, n_channels=None  ->  (10, 11, 12)\n",
    "    \"\"\"\n",
    "\n",
    "    shape = __dim_voxels(n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "    if n_samples is not None:\n",
    "        shape = (n_samples,) + shape\n",
    "    if n_channels is not None:\n",
    "        shape = shape + (n_channels,)\n",
    "\n",
    "    return shape\n",
    "\n",
    "\n",
    "def initialize_image_array(n_voxels, n_dim=None, n_samples=None, n_channels=None,\n",
    "                           dtype=bool, initialization='zeros'):\n",
    "    shape = image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_samples=n_samples, n_channels=n_channels)\n",
    "    return initialize_array(shape=shape, mode=initialization, dtype=dtype)\n",
    "\n",
    "\n",
    "# Image Compression <-> Decompression\n",
    "def img2compressed(img, n_dim=-1, level=9):\n",
    "    \"\"\"\n",
    "    Compress the given image with the zlib routine to a binary string.\n",
    "    Level of compression can be adjusted. A timing with respect to different compression levels for decompression showed\n",
    "    no difference, so the highest level is default, this corresponds to the largest compression.\n",
    "    For compression it is slightly slower but this happens just once and not during keras training, so the smaller\n",
    "    needed memory was favoured.\n",
    "    Alternative:\n",
    "    <-> use numpy sparse for the world images, especially in 3d  -> zlib is more effective and more general\n",
    "    \"\"\"\n",
    "\n",
    "    if n_dim == -1:\n",
    "        return zlib.compress(img.tobytes(), level=level)\n",
    "    else:\n",
    "        shape = img.shape[:-n_dim]\n",
    "        img_cmp = np.empty(shape, dtype=object)\n",
    "        for idx in np.ndindex(*shape):\n",
    "            img_cmp[idx] = zlib.compress(img[idx, ...].tobytes(), level=level)\n",
    "        return img_cmp\n",
    "\n",
    "\n",
    "def compressed2img(img_cmp, n_voxels, n_dim=None, n_channels=None, dtype=bool):\n",
    "    \"\"\"\n",
    "    Decompress the binary string back to an image of given shape\n",
    "    \"\"\"\n",
    "\n",
    "    shape = np.shape(img_cmp)\n",
    "\n",
    "    if shape:\n",
    "        n_samples = np.size(img_cmp)\n",
    "        img_arr = initialize_image_array(n_voxels=n_voxels, n_dim=n_dim, n_samples=n_samples, n_channels=n_channels,\n",
    "                                         dtype=dtype)\n",
    "        for i in range(n_samples):\n",
    "            img_arr[i, ...] = np.fromstring(zlib.decompress(img_cmp[i]), dtype=dtype).reshape(\n",
    "                image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_channels=n_channels))\n",
    "        return img_arr\n",
    "\n",
    "    else:\n",
    "        return np.fromstring(zlib.decompress(img_cmp), dtype=dtype).reshape(\n",
    "            image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_channels=n_channels))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class create_dataset(tf.keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "            y[j] -= 1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a23508",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfb5ed6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3254/1979238740.py:277: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_arr[i, ...] = np.fromstring(zlib.decompress(img_cmp[i]), dtype=dtype).reshape(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file = '../SingleSphere02.db'\n",
    "# TODO change to you own directory\n",
    "\n",
    "\n",
    "n_voxels = 64\n",
    "voxel_size = 10 / 64     # in m\n",
    "extent = [0, 10, 0, 10]  # in m\n",
    "n_waypoints = 22  # start + 20 inner points + end\n",
    "n_dim = 2\n",
    "n_paths_per_world = 1000\n",
    "n_worlds = 5000\n",
    "\n",
    "\n",
    "worlds = get_values_sql(file=file, table='worlds')\n",
    "obstacle_images = compressed2img(img_cmp=worlds.obst_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "# always 1000 paths belong to one world\n",
    "# 0...999     -> world 0\n",
    "# 1000...1999 -> world 1\n",
    "# 2000...2999 -> world 2\n",
    "paths = get_values_sql(file=file, table='paths', rows=[0, 1, 2, 1000, 2000])\n",
    "path_images = compressed2img(img_cmp=paths.path_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "start_images = compressed2img(img_cmp=paths.start_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "end_images = compressed2img(img_cmp=paths.end_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "q_paths = object2numeric_array(paths.q_path.values)\n",
    "q_paths = q_paths.reshape(-1, n_waypoints, n_dim)\n",
    "\n",
    "# Plot an example\n",
    "i = 0\n",
    "i_world = paths.i_world[i]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(obstacle_images[i_world].T, origin='lower', extent=extent, cmap='binary',)\n",
    "ax.imshow(start_images[i].T, origin='lower', extent=extent, cmap='Greens', alpha=0.4)\n",
    "ax.imshow(end_images[i].T, origin='lower', extent=extent, cmap='Reds', alpha=0.4)\n",
    "ax.imshow(path_images[i].T, origin='lower', extent=extent, cmap='Blues', alpha=0.2)\n",
    "ax.axis('off')\n",
    "ax.plot(*q_paths[i].T, color='k', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea6a80",
   "metadata": {},
   "source": [
    "# Creating Image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe61c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_for_single_world(world_index, file='../SingleSphere02.db', paths_per_world=1000):\n",
    "    \n",
    "    \"\"\" Returns an array of tuples where each tuple contains:\n",
    "        1. Obstacle image of the desired world with different start & end points\n",
    "        2. Above image plus the path from start to end point\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial Parameters\n",
    "    n_voxels = 64\n",
    "    voxel_size = 10 / 64     # in m\n",
    "    extent = [0, 10, 0, 10]  # in m\n",
    "    n_waypoints = 22  # start + 20 inner points + end\n",
    "    n_dim = 2\n",
    "    n_paths_per_world = 1000\n",
    "    n_worlds = 5000\n",
    "\n",
    "    worlds = get_values_sql(file=file, table='worlds')\n",
    "    obstacle_image = compressed2img(img_cmp=worlds.obst_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)[world_index]\n",
    "    \n",
    "    # always 1000 paths belong to one world\n",
    "    # 0...999     -> world 0\n",
    "    # 1000...1999 -> world 1\n",
    "    # 2000...2999 -> world 2\n",
    "    path_range_start  = world_index * paths_per_world\n",
    "    path_range_end = world_index * paths_per_world + paths_per_world\n",
    "    n_world_all_paths = [x for x in range(path_range_start, path_range_end)]\n",
    "    \n",
    "    paths = get_values_sql(file=file, table='paths', rows=n_world_all_paths)\n",
    "    \n",
    "    # Decompressing objects to images\n",
    "    path_images = compressed2img(img_cmp=paths.path_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "    start_images = compressed2img(img_cmp=paths.start_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "    end_images = compressed2img(img_cmp=paths.end_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "    input_images = []\n",
    "    output_images = []\n",
    "    for i in n_world_all_paths:\n",
    "        input_images.append(np.concatenate((obstacle_images[i_world].T[:,:,np.newaxis], start_images[i].T[:,:,np.newaxis],end_images[i].T[:,:,np.newaxis]), axis=-1))\n",
    "        #input_images.append(obstacle_images[i_world].T + start_images[i].T + end_images[i].T)\n",
    "        output_images.append(np.expand_dims(path_images[i].T, axis=2))\n",
    "    return input_images, output_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24b8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3254/1979238740.py:277: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_arr[i, ...] = np.fromstring(zlib.decompress(img_cmp[i]), dtype=dtype).reshape(\n"
     ]
    }
   ],
   "source": [
    "input_images, output_images = load_images_for_single_world(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ca5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Images\n",
    "def plot_image(image_array):\n",
    "    ''' Displays numpy arrays'''\n",
    "    return tf.keras.utils.array_to_img(tf.image.resize(image_array.astype('uint8'), (256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff7563f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 01:09:17.718954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-13 01:09:17.719038: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-13 01:09:17.719106: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2cc49966f54a): /proc/driver/nvidia/version does not exist\n",
      "2021-12-13 01:09:17.719737: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAFRUlEQVR4nO2dvW7qQBBGJ8KFCwoKChcULnmEvH91H4HSBVfagoICpC2MdIuEmyjxrvdnZpdZfae2jXWk+TwzwYQIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEC7vMldetP3fRd47GytfcjdiofQO0ygH4ZhG3jszRhzl7sVD6ICjsd94LGXE11bFPB+CDz2TNdJ7k58CAh41v5hHMdQAZ0xhqrkgICAZ+3vj0Mffs6Rhio5ICLgo/a3Q4wA2g1VckBGwPuBiLq+jxCwG4cqOSAgoNvuD2P0OVsiY0xHpXsCwadALP1wpAuV7gleSgDtblS6J3gpAbtxptI9AbOATd/3h/026ard9rNxTjw/DeaP6odhGCOe//VhF3A8jhHP//rwC3gfI57/9WEW0G33h9D+33eNcr3ACz0FnpSdC15SQMm54CUFlJwLGAW4eoDZWju7Pn5hb/icC8rsBxgFuHoAa4y5Oc7ZOvaG5XKAVcByD2DN6XRxnLM/0m5ZQKkc4BWw2ANYc/pzdpxzoN24fK1SOcAkYNP3/biwA5yttdM0TX8d583DMFDVHGAS4Kv/6WSs6zxrTmSq5gCbAHf9T8YngK6mag7wCXDW/2StR8B1MlVzgEGAq/6JiObb5eyqfyKix/1O5MmBWXw3wHD53B2ALwfkYRGQtwPw5YA8PAKydgC+HJAnU4C3/q2158vNNQb853G/U+c4Tn43kCnAV/9rPUDY9aV7gWwB7vpf6wHCri/dC+QLcNb/Wg8Qdn3pXiBDgK/+idZ7gJ/H1tkNZAjg/BtAvZkgSwDf3wDqzQR5Atj+BlBvJkgUsFr/gT3Ak7WZQC4HEgWs1X9KD1AnB5IF+Os/pQeokwPpArz1n9ID1MmBBAEh9e/bA7qosxtIuKRE/X+dW3o3kCSAv/6/zi29G0gTwF7/X+eW3g1EClib/2ciOifU/xN/DkjsBiIFrM3/H19zyxuBXTkg0wtEC/DP/59fdMwUsJwDMr1AvADv/H+mj1LIuSVXDsj0AhEC1uo/5dm/hGtHKDMTRAiQ3v+F3QN3DkQJkN3/hd0Ddw7ECRDd/4XdA3cOBAooVf//r1lsRxgooHT9l9sNBAsoW//ldgPhAorWf7ndQKAA13eAJeqfqOSOMHPFIPn8L5MD2QLknv9lciBfgNjzv0wOBApYei5L1f+TMjvCwEss1WOJ/l9+Rxgs4Hc9luj/5XeEwQJ+12OJ/l9+Rxgo4FmP34f0nN1fKPLfH4qIEWtO9P0nMXJ3f7nw9AJRAuj6vRZzd3+58PQCUQKu08/HYNZnZ8LTC0QIeNzr/M6L7G7gBd8a+4nsTKBCgORMoEKA5EygQIDsbkCBACLJHFAjQCoH1AiQygElAuRyQIkAIqkcUCVAIgdUCZDIAUUCZHaEigQQSewI1Qng3hGqE8C9I1QmgH9HqEyAj7ReoCkBKb1AUwJSegGFAnh3hAoF8M4EKgVwzgQqBXDOBAoF8O4GFAog4swBtQK4ckCtAK4cUCqALweUCiDiygHVAjhyQLUAjhxQLIBnR6hYABHHjlC9gNwdoXoBuTtC5QLyd4TKBfgI6wWaFhDSCzQtIKQXaECAb0e43gs0ICCvF2hCQE4v0ISAnF6gAQG+mWCdBgQQ5eRAMwJSc6AZAak50IiA9BxoRABRag40JSAlB5oSkJIDDQlYfLdx9XdNGxJAlPJuY3MCYt9tfBO9n+JsFl7xL//vvAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACABP8AMPNiwHkJ64sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x7FE46F4D9E80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_images[0].shape)\n",
    "plot_image(output_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59c76c",
   "metadata": {},
   "source": [
    "# Creating tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69ed427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_np_arrays(list_of_images):\n",
    "    ''' Returns a singular numpy array from a list of images. \\\n",
    "        It is used to arrange images to be compatible while making tensorflow datasets\n",
    "    '''\n",
    "    \n",
    "    length = list_of_images[0].shape[0]\n",
    "    width  = list_of_images[0].shape[1]\n",
    "    depth  = list_of_images[0].shape[2]\n",
    "    \n",
    "    imgs   = np.empty((0, length, width, depth))\n",
    "    \n",
    "    for img in list_of_images:\n",
    "        imgs = np.append(imgs, np.array(img).reshape((1, length, width, depth)), axis=0)\n",
    "        \n",
    "    return imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b5cbf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Images Shape after Reshaping:  (1000, 64, 64, 3)\n",
      "Output Images Shape after Reshaping:  (1000, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "input_images = np.reshape(input_images, (-1, 64, 64, 3))\n",
    "print(\"Input Images Shape after Reshaping: \",input_images.shape)\n",
    "output_images = np.reshape(output_images, (-1, 64, 64, 1))\n",
    "print(\"Output Images Shape after Reshaping: \",output_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090e9fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAFRUlEQVR4nO2dvW7qQBBGJ8KFCwoKChcULnmEvH91H4HSBVfagoICpC2MdIuEmyjxrvdnZpdZfae2jXWk+TwzwYQIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEC7vMldetP3fRd47GytfcjdiofQO0ygH4ZhG3jszRhzl7sVD6ICjsd94LGXE11bFPB+CDz2TNdJ7k58CAh41v5hHMdQAZ0xhqrkgICAZ+3vj0Mffs6Rhio5ICLgo/a3Q4wA2g1VckBGwPuBiLq+jxCwG4cqOSAgoNvuD2P0OVsiY0xHpXsCwadALP1wpAuV7gleSgDtblS6J3gpAbtxptI9AbOATd/3h/026ard9rNxTjw/DeaP6odhGCOe//VhF3A8jhHP//rwC3gfI57/9WEW0G33h9D+33eNcr3ACz0FnpSdC15SQMm54CUFlJwLGAW4eoDZWju7Pn5hb/icC8rsBxgFuHoAa4y5Oc7ZOvaG5XKAVcByD2DN6XRxnLM/0m5ZQKkc4BWw2ANYc/pzdpxzoN24fK1SOcAkYNP3/biwA5yttdM0TX8d583DMFDVHGAS4Kv/6WSs6zxrTmSq5gCbAHf9T8YngK6mag7wCXDW/2StR8B1MlVzgEGAq/6JiObb5eyqfyKix/1O5MmBWXw3wHD53B2ALwfkYRGQtwPw5YA8PAKydgC+HJAnU4C3/q2158vNNQb853G/U+c4Tn43kCnAV/9rPUDY9aV7gWwB7vpf6wHCri/dC+QLcNb/Wg8Qdn3pXiBDgK/+idZ7gJ/H1tkNZAjg/BtAvZkgSwDf3wDqzQR5Atj+BlBvJkgUsFr/gT3Ak7WZQC4HEgWs1X9KD1AnB5IF+Os/pQeokwPpArz1n9ID1MmBBAEh9e/bA7qosxtIuKRE/X+dW3o3kCSAv/6/zi29G0gTwF7/X+eW3g1EClib/2ciOifU/xN/DkjsBiIFrM3/H19zyxuBXTkg0wtEC/DP/59fdMwUsJwDMr1AvADv/H+mj1LIuSVXDsj0AhEC1uo/5dm/hGtHKDMTRAiQ3v+F3QN3DkQJkN3/hd0Ddw7ECRDd/4XdA3cOBAooVf//r1lsRxgooHT9l9sNBAsoW//ldgPhAorWf7ndQKAA13eAJeqfqOSOMHPFIPn8L5MD2QLknv9lciBfgNjzv0wOBApYei5L1f+TMjvCwEss1WOJ/l9+Rxgs4Hc9luj/5XeEwQJ+12OJ/l9+Rxgo4FmP34f0nN1fKPLfH4qIEWtO9P0nMXJ3f7nw9AJRAuj6vRZzd3+58PQCUQKu08/HYNZnZ8LTC0QIeNzr/M6L7G7gBd8a+4nsTKBCgORMoEKA5EygQIDsbkCBACLJHFAjQCoH1AiQygElAuRyQIkAIqkcUCVAIgdUCZDIAUUCZHaEigQQSewI1Qng3hGqE8C9I1QmgH9HqEyAj7ReoCkBKb1AUwJSegGFAnh3hAoF8M4EKgVwzgQqBXDOBAoF8O4GFAog4swBtQK4ckCtAK4cUCqALweUCiDiygHVAjhyQLUAjhxQLIBnR6hYABHHjlC9gNwdoXoBuTtC5QLyd4TKBfgI6wWaFhDSCzQtIKQXaECAb0e43gs0ICCvF2hCQE4v0ISAnF6gAQG+mWCdBgQQ5eRAMwJSc6AZAak50IiA9BxoRABRag40JSAlB5oSkJIDDQlYfLdx9XdNGxJAlPJuY3MCYt9tfBO9n+JsFl7xL//vvAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACABP8AMPNiwHkJ64sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x7FE46F94FA30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_image(output_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c160c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list_to_np_arrays(input_images)\n",
    "y = list_to_np_arrays(output_images)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118802cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numpy arrays to tf Dataset\n",
    "train_images_ds = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.cast(data_train, tf.float32),\n",
    "            tf.cast(labels_train, tf.float32)\n",
    "        ))\n",
    "\n",
    "test_images_ds = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.cast(data_test, tf.float32),\n",
    "            tf.cast(labels_test, tf.float32)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c791168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot_image(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a65e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256 at 0x7FE46F4E4F10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = train_images_ds.take(2)\n",
    "print(\"\",a[0].shape)\n",
    "a,b = train_images_ds.take(2)\n",
    "tf.keras.utils.array_to_img(tf.image.resize(b[0], (256, 256)))\n",
    "#plot_image(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0f203",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68011bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(img_size, num_classes):\n",
    "    inputs = tf.keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(3, 3,  padding=\"same\")(x)\n",
    "    outputs = layers.Flatten()(outputs)\n",
    "#     outputs = layers.Dense(12288, activation=\"relu\")(outputs)\n",
    "    outputs = layers.Dense(8192, activation=\"relu\")(outputs)\n",
    "    outputs = layers.Dense(4096, activation=\"softmax\")(outputs)\n",
    "    \n",
    "    outputs = layers.Reshape((64, 64, 1))(outputs)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "951d2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for the loader\n",
    "TRAIN_LENGTH = data_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "img_size = (64,64)\n",
    "\n",
    "#train_images_ds = train_images_ds.map(resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Building input pipeline\n",
    "train_batches = (\n",
    "    train_images_ds\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "test_batches = test_images_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "850a2988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 32, 32, 32)   896         ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 32, 32, 32)  128         ['conv2d_72[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_120 (Activation)    (None, 32, 32, 32)   0           ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " activation_121 (Activation)    (None, 32, 32, 32)   0           ['activation_120[0][0]']         \n",
      "                                                                                                  \n",
      " separable_conv2d_48 (Separable  (None, 32, 32, 64)  2400        ['activation_121[0][0]']         \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 32, 32, 64)  256         ['separable_conv2d_48[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_122 (Activation)    (None, 32, 32, 64)   0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_49 (Separable  (None, 32, 32, 64)  4736        ['activation_122[0][0]']         \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 32, 32, 64)  256         ['separable_conv2d_49[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d_24 (MaxPooling2D  (None, 16, 16, 64)  0           ['batch_normalization_122[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 64)   2112        ['activation_120[0][0]']         \n",
      "                                                                                                  \n",
      " add_56 (Add)                   (None, 16, 16, 64)   0           ['max_pooling2d_24[0][0]',       \n",
      "                                                                  'conv2d_73[0][0]']              \n",
      "                                                                                                  \n",
      " activation_123 (Activation)    (None, 16, 16, 64)   0           ['add_56[0][0]']                 \n",
      "                                                                                                  \n",
      " separable_conv2d_50 (Separable  (None, 16, 16, 128)  8896       ['activation_123[0][0]']         \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 16, 16, 128)  512        ['separable_conv2d_50[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_124 (Activation)    (None, 16, 16, 128)  0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_51 (Separable  (None, 16, 16, 128)  17664      ['activation_124[0][0]']         \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 16, 16, 128)  512        ['separable_conv2d_51[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d_25 (MaxPooling2D  (None, 8, 8, 128)   0           ['batch_normalization_124[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 8, 8, 128)    8320        ['add_56[0][0]']                 \n",
      "                                                                                                  \n",
      " add_57 (Add)                   (None, 8, 8, 128)    0           ['max_pooling2d_25[0][0]',       \n",
      "                                                                  'conv2d_74[0][0]']              \n",
      "                                                                                                  \n",
      " activation_125 (Activation)    (None, 8, 8, 128)    0           ['add_57[0][0]']                 \n",
      "                                                                                                  \n",
      " separable_conv2d_52 (Separable  (None, 8, 8, 256)   34176       ['activation_125[0][0]']         \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 8, 8, 256)   1024        ['separable_conv2d_52[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_126 (Activation)    (None, 8, 8, 256)    0           ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_53 (Separable  (None, 8, 8, 256)   68096       ['activation_126[0][0]']         \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 8, 8, 256)   1024        ['separable_conv2d_53[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d_26 (MaxPooling2D  (None, 4, 4, 256)   0           ['batch_normalization_126[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 4, 4, 256)    33024       ['add_57[0][0]']                 \n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 4, 4, 256)    0           ['max_pooling2d_26[0][0]',       \n",
      "                                                                  'conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " activation_127 (Activation)    (None, 4, 4, 256)    0           ['add_58[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_64 (Conv2DTra  (None, 4, 4, 256)   590080      ['activation_127[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 4, 4, 256)   1024        ['conv2d_transpose_64[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_128 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_transpose_65 (Conv2DTra  (None, 4, 4, 256)   590080      ['activation_128[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_128 (Batch  (None, 4, 4, 256)   1024        ['conv2d_transpose_65[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " up_sampling2d_65 (UpSampling2D  (None, 8, 8, 256)   0           ['add_58[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_64 (UpSampling2D  (None, 8, 8, 256)   0           ['batch_normalization_128[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 8, 8, 256)    65792       ['up_sampling2d_65[0][0]']       \n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 8, 8, 256)    0           ['up_sampling2d_64[0][0]',       \n",
      "                                                                  'conv2d_76[0][0]']              \n",
      "                                                                                                  \n",
      " activation_129 (Activation)    (None, 8, 8, 256)    0           ['add_59[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_66 (Conv2DTra  (None, 8, 8, 128)   295040      ['activation_129[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_129 (Batch  (None, 8, 8, 128)   512         ['conv2d_transpose_66[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_130 (Activation)    (None, 8, 8, 128)    0           ['batch_normalization_129[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_transpose_67 (Conv2DTra  (None, 8, 8, 128)   147584      ['activation_130[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 8, 8, 128)   512         ['conv2d_transpose_67[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " up_sampling2d_67 (UpSampling2D  (None, 16, 16, 256)  0          ['add_59[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_66 (UpSampling2D  (None, 16, 16, 128)  0          ['batch_normalization_130[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 16, 16, 128)  32896       ['up_sampling2d_67[0][0]']       \n",
      "                                                                                                  \n",
      " add_60 (Add)                   (None, 16, 16, 128)  0           ['up_sampling2d_66[0][0]',       \n",
      "                                                                  'conv2d_77[0][0]']              \n",
      "                                                                                                  \n",
      " activation_131 (Activation)    (None, 16, 16, 128)  0           ['add_60[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_68 (Conv2DTra  (None, 16, 16, 64)  73792       ['activation_131[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 16, 16, 64)  256         ['conv2d_transpose_68[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_132 (Activation)    (None, 16, 16, 64)   0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_transpose_69 (Conv2DTra  (None, 16, 16, 64)  36928       ['activation_132[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 16, 16, 64)  256         ['conv2d_transpose_69[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " up_sampling2d_69 (UpSampling2D  (None, 32, 32, 128)  0          ['add_60[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_68 (UpSampling2D  (None, 32, 32, 64)  0           ['batch_normalization_132[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 32, 32, 64)   8256        ['up_sampling2d_69[0][0]']       \n",
      "                                                                                                  \n",
      " add_61 (Add)                   (None, 32, 32, 64)   0           ['up_sampling2d_68[0][0]',       \n",
      "                                                                  'conv2d_78[0][0]']              \n",
      "                                                                                                  \n",
      " activation_133 (Activation)    (None, 32, 32, 64)   0           ['add_61[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_70 (Conv2DTra  (None, 32, 32, 32)  18464       ['activation_133[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_133 (Batch  (None, 32, 32, 32)  128         ['conv2d_transpose_70[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_134 (Activation)    (None, 32, 32, 32)   0           ['batch_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_transpose_71 (Conv2DTra  (None, 32, 32, 32)  9248        ['activation_134[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_134 (Batch  (None, 32, 32, 32)  128         ['conv2d_transpose_71[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " up_sampling2d_71 (UpSampling2D  (None, 64, 64, 64)  0           ['add_61[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_70 (UpSampling2D  (None, 64, 64, 32)  0           ['batch_normalization_134[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 64, 64, 32)   2080        ['up_sampling2d_71[0][0]']       \n",
      "                                                                                                  \n",
      " add_62 (Add)                   (None, 64, 64, 32)   0           ['up_sampling2d_70[0][0]',       \n",
      "                                                                  'conv2d_79[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 64, 64, 3)    867         ['add_62[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 12288)        0           ['conv2d_80[0][0]']              \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 8192)         100671488   ['flatten_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 4096)         33558528    ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 64, 64, 1)    0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 136,288,995\n",
      "Trainable params: 136,285,219\n",
      "Non-trainable params: 3,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# lr = 0.001\n",
    "# loss_fn = tf.reduce_mean(tf.square(ae_outputs - ae_target))\n",
    "# train_op = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss_fn)\n",
    "\n",
    "model = get_model(img_size, 3)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = tf.keras.losses.mean_squared_error,\n",
    "              metrics = ['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3fc7102c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "25/25 [==============================] - 1700s 60s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0582 - val_mse: 0.0582\n",
      "Epoch 2/18\n",
      "25/25 [==============================] - 64s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0582 - val_mse: 0.0582\n",
      "Epoch 3/18\n",
      "25/25 [==============================] - 64s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0584 - val_mse: 0.0584\n",
      "Epoch 4/18\n",
      "25/25 [==============================] - 59s 2s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0584 - val_mse: 0.0584\n",
      "Epoch 5/18\n",
      "25/25 [==============================] - 63s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0584 - val_mse: 0.0584\n",
      "Epoch 6/18\n",
      "25/25 [==============================] - 61s 2s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0584 - val_mse: 0.0584\n",
      "Epoch 7/18\n",
      "25/25 [==============================] - 63s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 8/18\n",
      "25/25 [==============================] - 66s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 9/18\n",
      "25/25 [==============================] - 65s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 10/18\n",
      "25/25 [==============================] - 63s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 11/18\n",
      "25/25 [==============================] - 64s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 12/18\n",
      "25/25 [==============================] - 63s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 13/18\n",
      "25/25 [==============================] - 63s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 14/18\n",
      "25/25 [==============================] - 65s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 15/18\n",
      "25/25 [==============================] - 63s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 16/18\n",
      "25/25 [==============================] - 64s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 17/18\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 18/18\n",
      "25/25 [==============================] - 67s 3s/step - loss: 0.0581 - mse: 0.0581 - val_loss: 0.0583 - val_mse: 0.0583\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 18\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = data_test.shape[0]//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
    "                          #steps_per_epoch=1,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_batches,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "837c9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "876eacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b17f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = y_pred[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc6bb8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAAt0lEQVR4nO3VsQ3CMBCG0QNBY4ZIhsgS7J6aOq5DQxE3NCzg+oyU9xb4rF8nOQIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAPJf85L08SrSjffPTHbf8ZJnmKepWP/npjgEDPOZliTXepx2gTMszYn/ll3uuox8w2oALaHWNWGvLL/cMGODYYo+6HfnlntN/gwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPy7H9MMHHMq3PDfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x7FE4041E67F0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_image(y_pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38360d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAAt0lEQVR4nO3VsQ3CMBCG0QNBY4ZIhsgS7J6aOq5DQxE3NCzg+oyU9xb4rF8nOQIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAPJf85L08SrSjffPTHbf8ZJnmKepWP/npjgEDPOZliTXepx2gTMszYn/ll3uuox8w2oALaHWNWGvLL/cMGODYYo+6HfnlntN/gwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPy7H9MMHHMq3PDfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x7FE4041B4FA0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.array_to_img(tf.image.resize(y_pred[4].astype('float32'), (256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efe21f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAFk0lEQVR4nO2dPW/qWBRF15tQuKCgoLhFCpf8hPf/q/kJlBSM5IKCAiQXIE2RkDBRIn887tlbw1kVRbAPK8r2vhfLgSRJkiRJkiRJkiRJkiRJkuR5+DX8Iy9N0yxmHPrS9/11xvtiGfHJmlLKcsahT13XnWe8L5ZRAjab9YxDH7Yc/y8Cfr/OOPSe427G24IZEPDSNE3btu0cAYuu626H982DAQFNKaXdlGbOoZuy4fD+2jcPBgVsNm2ZK4DV6f21bx4MC/jdNs1MAav28v7aNw8GBCyW69c5f/9v7/28ePrmwZyGMwPfPAgT4JoHYQJc82BAwOV02C9mrgX+cxrbPBj4ZH23pZu5FvgJrzwYFMCx27B6rACnPBgUcNx1rNpHntIrDwYEXM9nKKXwgBz4OOU3eaDLghGfqkYO3LjlgS4LRgl4fA7cuOWBLgtGCXh8Dty45YEuC0YIuOXAZfhHPw87MjNueaDLgpHJ1ndbpmyLLSdmhi4LRgvgOOUDrSdmhi4LRgs47qZcBl8nZoYuC0Z+qut52u/lcpcZY/Lgoxusl0HLs48z1znsfWZMzYNYqgn4zIypeRBLNQGfmTE1D2KpJOA+M6bkwWK5fo3tAgGRMyUPmrKhhHaBEAHj86AprEpoFwgRMD4PmrJqS2gXCBAwJQ8WyyWX0C4QXDv8+kG4ALd+EC7ArR8EC/iaB4/ca5yH8Ow19xrHIxVQb69xPFIB9fYaxyMUcD2fWRxOU7YaK/CX9vR6UoB6ADUpQD2AmhSgHkBNClAPoCYFqAdQkwLUA6hJAeoB1KQA9QBqUoB6ADUpQD2AmhSgHkBNClAPoCYFqAdQI/xu8KVpmtfwe4O/Ijz9nzyb4HFIBcx/NsHj0AqY/WyCxyEU8N2zCS593+9D7xkQR9BX+q7rdtuujzujnYDtdtc9t4C/d33/vAIup8P+n9AzPn0TTAHqAdSkAPUAalKAegA1oh7gsRcAMgEeewEgFOCwFwBKAQZ7ASAT4LEXAFaLofi9ADATEL0XAG4CgvcCQCLgpw4QvxcAEgE+HQBEAlw6AKgEmHQAkAj4k2cVP56nXw2mAPUAakyKkGYdADYCNOsAMBKgWAeAkwDBOgDCBXitAyBcgNc6AAQCnNYBoBBgtA6AcAE+e4E3DK4Cug4AJgJUHQBcBIg6AIQK8OsAECrArwNAsAC3DgDRAsw6AIQK8OsAIL8KaDsAGAhQdgBwECDsABAmwLMDQOB/m3PsABD5DxcNOwBECjDsABAmwOv7wHue/puhFKAeQI2wCOnXASB+srR6HQDqh6uL1wGgFiBeB0CIAN91AIQI8F0HQJAA13UARAkwXQdAiADPvcAboquARwcAoQCHDgBKAQYdAKoL8O4AUF2AdweAAAHOHQAiBBh3AKguwLsDgOQq4NMBQCTApQOASoBJB4CqAvw7AFQV4N8BoLIA9w4AtQWYdwCoKsD3+8B7nv6boRSgHkBNpQz4sQNYrQOgmoCfOoDXOgAqCvi+A3itA6CmgG87gNc6AKoJ+KkDeK0DIK8CKeDpBQRuiPh1AAgV4NcBIFiAWweAaAFmHQBiM8CuA0BeBVJAClAPoCYFqAdQkwLUA6hJAeoB1Dy9gKC1gOdeAIQJ8NwLgEABjnsBECnAcC8A4jLAci8A8iqQAlKAegA1KUA9gJoUoB5ATQpQD6AmBagHUJMC1AOoSQHqAdSkAPUAalKAegA1KUA9gJoUoB5ATQpQD6AmBagHUJMC1AOoeXoBle4PuJwO+0XTNAuc7w+CagL6bktXSlnifH8QVBTAsduwehPgen8QVBRw3HWs2rfXrvcHQTUB1/MZSikXYL/b7TzvD4KqN0n13ZY1cLD9+4fKAjgugZPt3z/Ar3qHfrm7DF7rnSZJkiRJkiRJZvEvGEjpBt4hl6wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x7FE444171580>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_image(labels_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0dc61",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64, 64, 3)\n",
    "\n",
    "x = np.reshape(data_test[4], (-1, 64, 64, 3))\n",
    "print(x.shape)\n",
    "net = tf.keras.layers.Conv2D(64, 2, input_shape=x[1:], activation = tf.nn.relu)\n",
    "print(\"After conv2d: \", net.shape)\n",
    "net = tf.keras.layers.MaxPooling2D(net, 2, 2, padding = 'same')\n",
    "\n",
    "print(net.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa442b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (64, 64, 3)\n",
    "x = data_test[4]\n",
    "y = tf.keras.layers.Conv1D(\n",
    "                3, 1, activation='relu', input_shape=input_shape[1:])(x)\n",
    "\n",
    "print((y).shape)\n",
    "tf.keras.utils.array_to_img(tf.image.resize(y-1, (256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "X_train = np.array(X[800:])\n",
    "X_test = np.array(X[:800])\n",
    "y_train = np.array(y[800:])\n",
    "y_test = np.array(y[:800])\n",
    "\n",
    "train_gen = OxfordPets(\n",
    "    batch_size, img_size, X_train, y_train\n",
    ")\n",
    "train_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee6f3e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "img_size = (64,64)\n",
    "num_classes = 3 \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 15\n",
    "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9168cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64,64,1)\n",
    "\n",
    "def gen_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Flatten(activation='relu')\n",
    "    tf.keras.layers.Dense(4096,activation='relu'),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dense(64*64, activation='relu'),\n",
    "    tf.keras.layers.Reshape((64, 64))\n",
    "        \n",
    "     ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.mean_squared_error,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96382c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abccf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d922008",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3141054",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451054b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Path Image\")\n",
    "plot_image(y_pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b873124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Real Path Image\")\n",
    "plot_image(y_test[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9de17a",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
