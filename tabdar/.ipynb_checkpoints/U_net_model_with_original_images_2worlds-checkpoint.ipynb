{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdae782",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730bd1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 05:23:53.854087: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-16 05:23:53.854149: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "import zlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "from contextlib import contextmanager\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "_CMP = '_cmp'\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def open_db_connection(*, file, close=True,\n",
    "                       lock=None, check_same_thread=False):\n",
    "    \"\"\"\n",
    "    Safety wrapper for the database call.\n",
    "    \"\"\"\n",
    "\n",
    "    if lock is not None:\n",
    "        lock.acquire()\n",
    "\n",
    "    con = sql.connect(database=file, check_same_thread=check_same_thread)\n",
    "\n",
    "    try:\n",
    "        yield con\n",
    "\n",
    "    finally:\n",
    "        if close:\n",
    "            con.close()\n",
    "        if lock is not None:\n",
    "            lock.release()\n",
    "\n",
    "\n",
    "def get_table_name(file):\n",
    "    with open_db_connection(file=file, close=True) as con:\n",
    "        res = pd.read_sql_query(sql=\"SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%'\",\n",
    "                                con=con)\n",
    "        return res['name'].values\n",
    "\n",
    "\n",
    "def rename_table(file, tables):\n",
    "    old_names = get_table_name(file=file)\n",
    "\n",
    "    with open_db_connection(file=file, close=True) as con:\n",
    "        cur = con.cursor()\n",
    "        for old in old_names:\n",
    "            if old in tables:\n",
    "                new = tables['old']\n",
    "                cur.execute(f\"ALTER TABLE `{old}` RENAME TO `{new}`\")\n",
    "\n",
    "\n",
    "def get_values_sql(*, file, table='db', columns=None, rows=-1,\n",
    "                   values_only=False, squeeze_col=True, squeeze_row=True):\n",
    "    \"\"\"\n",
    "    'i_samples' == i_samples_global\n",
    "    \"\"\"\n",
    "\n",
    "    lock = None  # Lock is not necessary fo reading\n",
    "    if columns is None:\n",
    "        columns = '*'\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    columns_str = ', '.join(map(str, columns))\n",
    "\n",
    "    if isinstance(rows, int):\n",
    "        rows = [rows]\n",
    "    rows = np.array(rows)\n",
    "\n",
    "    if rows[0] == -1:  # All samples\n",
    "        with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "            df = pd.read_sql_query(con=con, sql=f\"SELECT {columns_str} FROM {table}\")  # path_db\n",
    "\n",
    "    else:\n",
    "        rows_str = rows + 1  # Attention! Unlike in Python, SQL indices start at 1\n",
    "        rows_str = ', '.join(map(str, rows_str))\n",
    "        with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "            df = pd.read_sql_query(sql=f\"SELECT {columns_str} FROM {table} WHERE ROWID in ({rows_str})\",\n",
    "                                   index_col=rows, con=con)\n",
    "\n",
    "    value_list = []\n",
    "    if np.any(columns == ['*']):\n",
    "        columns = df.columns.values\n",
    "\n",
    "    if values_only:\n",
    "        for col in columns:\n",
    "            value = __decompress_values(value=df.loc[:, col].values, col=col)\n",
    "            value_list.append(value)\n",
    "\n",
    "        if len(df) == 1 and squeeze_row:\n",
    "            for i in range(len(columns)):\n",
    "                value_list[i] = value_list[i][0]\n",
    "\n",
    "        if len(value_list) == 1 and squeeze_col:\n",
    "            value_list = value_list[0]\n",
    "\n",
    "        return value_list\n",
    "\n",
    "    # Return pandas.DataFrame\n",
    "    else:\n",
    "        for col in columns:\n",
    "            value = __decompress_values(value=df.loc[:, col].values, col=col)\n",
    "            df.loc[:, col] = numeric2object_array(value)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def set_values_sql(*, file, table='db',\n",
    "                   values, columns, rows=-1, lock=None):\n",
    "    \"\"\"\n",
    "    Note: multidimensional numpy arrays have to be saved as flat to SQL otherwise the order is messed up\n",
    "    values = ([...], [...], [...], ...)\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle rows argument\n",
    "    if isinstance(rows, int):\n",
    "        if rows == -1:\n",
    "            rows = np.arange(len(values[0])).tolist()\n",
    "        else:\n",
    "            rows = [rows]\n",
    "\n",
    "    rows_sql = (np.array(rows) + 1).tolist()  # Attention! Unlike in Python, SQL indices start at 1\n",
    "\n",
    "    # Handle columns argument\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    columns_str = '=?, '.join(map(str, columns))\n",
    "    columns_str += '=?'\n",
    "\n",
    "    values_rows_sql = change_tuple_order(values + (rows_sql,))\n",
    "    values_rows_sql = list(values_rows_sql)\n",
    "    query = f\"UPDATE {table} SET {columns_str} WHERE ROWID=?\"\n",
    "\n",
    "    with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "        cur = con.cursor()\n",
    "        if len(values_rows_sql) == 1:\n",
    "            cur.execute(query, values_rows_sql[0])\n",
    "        else:\n",
    "            cur.executemany(query, values_rows_sql)\n",
    "\n",
    "        con.commit()\n",
    "\n",
    "\n",
    "def df2sql(df, file, table='db', if_exists='fail', lock=None):\n",
    "    \"\"\"\n",
    "    From DataFrame.to_sql():\n",
    "        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n",
    "                   - fail: If table exists, do nothing.\n",
    "                   - replace: If table exists, drop it, recreate it, and insert Measurements.\n",
    "                   - append: If table exists, insert Measurements. Create if does not exist.\n",
    "    \"\"\"\n",
    "    with open_db_connection(file=file, close=True, lock=lock) as con:\n",
    "        df.to_sql(name=table, con=con, if_exists=if_exists, index=False)\n",
    "\n",
    "\n",
    "# Helper\n",
    "# Image Compression <-> Decompression\n",
    "def __decompress_values(value, col):\n",
    "    # SQL saves everything in binary form -> convert back to numeric, expect the columns which are marked as CMP\n",
    "    if isinstance(value[0], bytes) and col[-4:] != _CMP:\n",
    "        if col in ['i_world', 'i_sample', 'n_obstacles']:\n",
    "            value = np.array([np.frombuffer(v, dtype=int) for v in value], dtype=int)\n",
    "        elif col in ['rectangle_pos', 'rectangle_position', 'rectangle_size']:\n",
    "            value = np.array([np.frombuffer(v, dtype=int) for v in value], dtype=object)\n",
    "        else:\n",
    "            value = np.array([np.frombuffer(v, dtype=float) for v in value])\n",
    "        value = np.squeeze(value)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def change_tuple_order(tpl):\n",
    "    return tuple(map(lambda *tt: tuple(tt), *tpl))\n",
    "\n",
    "\n",
    "def numeric2object_array(arr):\n",
    "    n = arr.shape[0]\n",
    "    arr_obj = np.zeros(n, dtype=object)\n",
    "    for i in range(n):\n",
    "        arr_obj[i] = arr[i]\n",
    "\n",
    "    return arr_obj\n",
    "\n",
    "\n",
    "def object2numeric_array(arr):\n",
    "    s = np.shape(arr)\n",
    "    arr = np.array([v for v in np.ravel(arr)])\n",
    "    arr = np.reshape(arr, s + np.shape(arr)[1:])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def initialize_array(shape, mode='zeros', dtype=None, order='c'):\n",
    "\n",
    "    if mode == 'zeros':\n",
    "        return np.zeros(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'ones':\n",
    "        return np.ones(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'empty':\n",
    "        return np.empty(shape, dtype=dtype, order=order)\n",
    "    elif mode == 'random':\n",
    "        return np.random.random(shape).astype(dtype=dtype, order=order)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown initialization method {mode}\")\n",
    "\n",
    "\n",
    "def __dim_voxels(n_voxels, n_dim=None):\n",
    "    if np.size(n_voxels) == 1:\n",
    "        try:\n",
    "            n_voxels = tuple(n_voxels)\n",
    "        except TypeError:\n",
    "            n_voxels = (n_voxels,)\n",
    "        n_voxels *= n_dim\n",
    "    else:\n",
    "        n_voxels = tuple(n_voxels)\n",
    "\n",
    "    return n_voxels\n",
    "\n",
    "\n",
    "def image_array_shape(n_voxels, n_samples=None, n_dim=None, n_channels=None):\n",
    "    \"\"\"\n",
    "    Helper to set the shape for an image array.\n",
    "    n_samples=100,  n_voxels=64,          n_dim=2,    n_channels=None  ->  (100, 64, 64)\n",
    "    n_samples=100,  n_voxels=64,          n_dim=3,    n_channels=2     ->  (100, 64, 64, 64, 2)\n",
    "    n_samples=None, n_voxel=(10, 11, 12), n_dim=None, n_channels=None  ->  (10, 11, 12)\n",
    "    \"\"\"\n",
    "\n",
    "    shape = __dim_voxels(n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "    if n_samples is not None:\n",
    "        shape = (n_samples,) + shape\n",
    "    if n_channels is not None:\n",
    "        shape = shape + (n_channels,)\n",
    "\n",
    "    return shape\n",
    "\n",
    "\n",
    "def initialize_image_array(n_voxels, n_dim=None, n_samples=None, n_channels=None,\n",
    "                           dtype=bool, initialization='zeros'):\n",
    "    shape = image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_samples=n_samples, n_channels=n_channels)\n",
    "    return initialize_array(shape=shape, mode=initialization, dtype=dtype)\n",
    "\n",
    "\n",
    "# Image Compression <-> Decompression\n",
    "def img2compressed(img, n_dim=-1, level=9):\n",
    "    \"\"\"\n",
    "    Compress the given image with the zlib routine to a binary string.\n",
    "    Level of compression can be adjusted. A timing with respect to different compression levels for decompression showed\n",
    "    no difference, so the highest level is default, this corresponds to the largest compression.\n",
    "    For compression it is slightly slower but this happens just once and not during keras training, so the smaller\n",
    "    needed memory was favoured.\n",
    "    Alternative:\n",
    "    <-> use numpy sparse for the world images, especially in 3d  -> zlib is more effective and more general\n",
    "    \"\"\"\n",
    "\n",
    "    if n_dim == -1:\n",
    "        return zlib.compress(img.tobytes(), level=level)\n",
    "    else:\n",
    "        shape = img.shape[:-n_dim]\n",
    "        img_cmp = np.empty(shape, dtype=object)\n",
    "        for idx in np.ndindex(*shape):\n",
    "            img_cmp[idx] = zlib.compress(img[idx, ...].tobytes(), level=level)\n",
    "        return img_cmp\n",
    "\n",
    "\n",
    "def compressed2img(img_cmp, n_voxels, n_dim=None, n_channels=None, dtype=bool):\n",
    "    \"\"\"\n",
    "    Decompress the binary string back to an image of given shape\n",
    "    \"\"\"\n",
    "\n",
    "    shape = np.shape(img_cmp)\n",
    "\n",
    "    if shape:\n",
    "        n_samples = np.size(img_cmp)\n",
    "        img_arr = initialize_image_array(n_voxels=n_voxels, n_dim=n_dim, n_samples=n_samples, n_channels=n_channels,\n",
    "                                         dtype=dtype)\n",
    "        for i in range(n_samples):\n",
    "            img_arr[i, ...] = np.fromstring(zlib.decompress(img_cmp[i]), dtype=dtype).reshape(\n",
    "                image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_channels=n_channels))\n",
    "        return img_arr\n",
    "\n",
    "    else:\n",
    "        return np.fromstring(zlib.decompress(img_cmp), dtype=dtype).reshape(\n",
    "            image_array_shape(n_voxels=n_voxels, n_dim=n_dim, n_channels=n_channels))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class create_dataset(tf.keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "            y[j] -= 1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a23508",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfb5ed6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528/3336820032.py:279: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_arr[i, ...] = np.fromstring(zlib.decompress(img_cmp[i]), dtype=dtype).reshape(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file = '../SingleSphere02.db'\n",
    "# TODO change to you own directory\n",
    "\n",
    "\n",
    "n_voxels = 64\n",
    "voxel_size = 10 / 64     # in m\n",
    "extent = [0, 10, 0, 10]  # in m\n",
    "n_waypoints = 22  # start + 20 inner points + end\n",
    "n_dim = 2\n",
    "n_paths_per_world = 1000\n",
    "n_worlds = 5000\n",
    "\n",
    "\n",
    "worlds = get_values_sql(file=file, table='worlds')\n",
    "obstacle_images = compressed2img(img_cmp=worlds.obst_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "# always 1000 paths belong to one world\n",
    "# 0...999     -> world 0\n",
    "# 1000...1999 -> world 1\n",
    "# 2000...2999 -> world 2\n",
    "paths = get_values_sql(file=file, table='paths', rows=[0, 1, 2, 1000, 2000])\n",
    "path_images = compressed2img(img_cmp=paths.path_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "start_images = compressed2img(img_cmp=paths.start_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "end_images = compressed2img(img_cmp=paths.end_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "q_paths = object2numeric_array(paths.q_path.values)\n",
    "q_paths = q_paths.reshape(-1, n_waypoints, n_dim)\n",
    "\n",
    "# Plot an example\n",
    "i = 0\n",
    "i_world = paths.i_world[i]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(obstacle_images[i_world].T, origin='lower', extent=extent, cmap='binary',)\n",
    "ax.imshow(start_images[i].T, origin='lower', extent=extent, cmap='Greens', alpha=0.4)\n",
    "ax.imshow(end_images[i].T, origin='lower', extent=extent, cmap='Reds', alpha=0.4)\n",
    "ax.imshow(path_images[i].T, origin='lower', extent=extent, cmap='Blues', alpha=0.2)\n",
    "ax.axis('off')\n",
    "ax.plot(*q_paths[i].T, color='k', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea6a80",
   "metadata": {},
   "source": [
    "# Creating Input/Output Image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe61c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_for_single_world(world_index, file='../SingleSphere02.db', paths_per_world=1000):\n",
    "    \n",
    "    \"\"\" Returns an array of tuples where each tuple contains:\n",
    "        1. Obstacle image of the desired world with different start & end points\n",
    "        2. Above image plus the path from start to end point\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial Parameters\n",
    "    n_voxels = 64\n",
    "    voxel_size = 10 / 64     # in m\n",
    "    extent = [0, 10, 0, 10]  # in m\n",
    "    n_waypoints = 22  # start + 20 inner points + end\n",
    "    n_dim = 2\n",
    "    n_paths_per_world = 1000\n",
    "    n_worlds = 5000\n",
    "\n",
    "    worlds = get_values_sql(file=file, table='worlds')\n",
    "    obstacle_image = compressed2img(img_cmp=worlds.obst_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)[world_index]\n",
    "    \n",
    "    # always 1000 paths belong to one world\n",
    "    # 0...999     -> world 0\n",
    "    # 1000...1999 -> world 1\n",
    "    # 2000...2999 -> world 2\n",
    "    path_range_start  = world_index * paths_per_world\n",
    "    path_range_end = world_index * paths_per_world + paths_per_world\n",
    "    n_world_all_paths = [x for x in range(path_range_start, path_range_end)]\n",
    "    \n",
    "    paths = get_values_sql(file=file, table='paths', rows=n_world_all_paths)\n",
    "    \n",
    "    # Decompressing objects to images\n",
    "    path_images = compressed2img(img_cmp=paths.path_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "    start_images = compressed2img(img_cmp=paths.start_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "    end_images = compressed2img(img_cmp=paths.end_img_cmp.values, n_voxels=n_voxels, n_dim=n_dim)\n",
    "\n",
    "    input_images = []\n",
    "    output_images = []\n",
    "    for i in range(len(n_world_all_paths)):\n",
    "        input_images.append(np.concatenate((obstacle_image.T[:,:,np.newaxis], start_images[i].T[:,:,np.newaxis],end_images[i].T[:,:,np.newaxis]), axis=-1))\n",
    "        #input_images.append(obstacle_images[i_world].T + start_images[i].T + end_images[i].T)\n",
    "        output_images.append(np.expand_dims(path_images[i].T, axis=-1))\n",
    "    return input_images, output_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f24b8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528/3336820032.py:279: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_arr[i, ...] = np.fromstring(zlib.decompress(img_cmp[i]), dtype=dtype).reshape(\n"
     ]
    }
   ],
   "source": [
    "input_images, output_images = load_images_for_single_world(0)\n",
    "input_images_1, output_images_1 = load_images_for_single_world(1)\n",
    "input_images_2, output_images_2 = load_images_for_single_world(2)\n",
    "\n",
    "input_images += input_images_1\n",
    "input_images += input_images_2\n",
    "\n",
    "output_images += output_images_1\n",
    "output_images += output_images_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5b6c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_images_1.insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaa229f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108408f",
   "metadata": {},
   "source": [
    "# Image Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a994b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_np_arrays(list_of_images):\n",
    "    ''' Returns a singular numpy array from a list of images. \\\n",
    "        It is used to arrange images to be compatible while making tensorflow datasets\n",
    "    '''\n",
    "    \n",
    "    length = list_of_images[0].shape[0]\n",
    "    width  = list_of_images[0].shape[1]\n",
    "    depth  = list_of_images[0].shape[2]\n",
    "    \n",
    "    imgs   = np.empty((0, length, width, depth))\n",
    "    \n",
    "    for img in list_of_images:\n",
    "        imgs = np.append(imgs, np.array(img).reshape((1, length, width, depth)), axis=0)\n",
    "        \n",
    "    return imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50542db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Images Shape after Reshaping:  (3000, 64, 64, 3)\n",
      "Output Images Shape after Reshaping:  (3000, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "input_images = np.reshape(input_images, (-1, 64, 64, 3))\n",
    "print(\"Input Images Shape after Reshaping: \",input_images.shape)\n",
    "\n",
    "output_images = np.reshape(output_images, (-1, 64, 64, 1))\n",
    "print(\"Output Images Shape after Reshaping: \",output_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8b4bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = input_images[:2000]\n",
    "data_test  = input_images[2000:]\n",
    "\n",
    "labels_train = output_images[:2000]\n",
    "labels_test  = output_images[2000:]\n",
    "\n",
    "labels_train = labels_train.astype(np.bool_)\n",
    "labels_test = labels_test.astype(np.bool_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f6530",
   "metadata": {},
   "source": [
    "# Sample Input and Output Data Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22341f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sample Output')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x936 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx=990\n",
    "path = labels_test[idx]\n",
    "path = path == 0\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(13,13))\n",
    "\n",
    "\n",
    "axs[0].imshow((data_test[idx]*255).astype(np.uint8),)\n",
    "axs[0].set_title('Sample Input')\n",
    "#axs[0].imshow((path*255).astype(np.uint8), cmap='Blues', alpha=0.2)\n",
    "\n",
    "axs[1].imshow((data_test[idx]*255).astype(np.uint8),)\n",
    "axs[1].imshow((path*255).astype(np.uint8), cmap='Blues', alpha=0.4)\n",
    "axs[1].set_title('Sample Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b48f8e",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbd23028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(img_size, num_classes):   \n",
    "    inputs = tf.keras.layers.Input(shape=img_size + (3,))\n",
    "    #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    #Expansive path \n",
    "    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0c16f",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66a5f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 05:33:21.696532: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-16 05:33:21.697221: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-16 05:33:21.697609: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2cc49966f54a): /proc/driver/nvidia/version does not exist\n",
      "2021-12-16 05:33:21.700539: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "img_size = (64,64)\n",
    "model = get_model(img_size, 3)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d0ce640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.2869 - accuracy: 0.9310\n",
      "Epoch 2/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.1624 - accuracy: 0.9410\n",
      "Epoch 3/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.1157 - accuracy: 0.9474\n",
      "Epoch 4/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0959 - accuracy: 0.9507\n",
      "Epoch 5/70\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.0797 - accuracy: 0.9637\n",
      "Epoch 6/70\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.0638 - accuracy: 0.9727\n",
      "Epoch 7/70\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.0553 - accuracy: 0.9767\n",
      "Epoch 8/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0532 - accuracy: 0.9779\n",
      "Epoch 9/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0458 - accuracy: 0.9813\n",
      "Epoch 10/70\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.0414 - accuracy: 0.9829\n",
      "Epoch 11/70\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.0404 - accuracy: 0.9835\n",
      "Epoch 12/70\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.0379 - accuracy: 0.9847\n",
      "Epoch 13/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0363 - accuracy: 0.9852\n",
      "Epoch 14/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0345 - accuracy: 0.9860\n",
      "Epoch 15/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0333 - accuracy: 0.9866\n",
      "Epoch 16/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0301 - accuracy: 0.9877\n",
      "Epoch 17/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0305 - accuracy: 0.9877\n",
      "Epoch 18/70\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.0289 - accuracy: 0.9884\n",
      "Epoch 19/70\n",
      "40/40 [==============================] - 66s 2s/step - loss: 0.0277 - accuracy: 0.9890\n",
      "Epoch 20/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0261 - accuracy: 0.9895\n",
      "Epoch 21/70\n",
      "40/40 [==============================] - 57s 1s/step - loss: 0.0258 - accuracy: 0.9895\n",
      "Epoch 22/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0247 - accuracy: 0.9899\n",
      "Epoch 23/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0242 - accuracy: 0.9902\n",
      "Epoch 24/70\n",
      "40/40 [==============================] - 56s 1s/step - loss: 0.0239 - accuracy: 0.9903\n",
      "Epoch 25/70\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.0223 - accuracy: 0.9911\n",
      "Epoch 26/70\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.0221 - accuracy: 0.9911\n",
      "Epoch 27/70\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.0209 - accuracy: 0.9915\n",
      "Epoch 28/70\n",
      "40/40 [==============================] - 57s 1s/step - loss: 0.0206 - accuracy: 0.9916\n",
      "Epoch 29/70\n",
      "40/40 [==============================] - 64s 2s/step - loss: 0.0206 - accuracy: 0.9917\n",
      "Epoch 30/70\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.0225 - accuracy: 0.9908\n",
      "Epoch 31/70\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.0211 - accuracy: 0.9916\n",
      "Epoch 32/70\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.0188 - accuracy: 0.9923\n",
      "Epoch 33/70\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.0188 - accuracy: 0.9925\n",
      "Epoch 34/70\n",
      "40/40 [==============================] - 75s 2s/step - loss: 0.0171 - accuracy: 0.9929\n",
      "Epoch 35/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0163 - accuracy: 0.9932\n",
      "Epoch 36/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0181 - accuracy: 0.9926\n",
      "Epoch 37/70\n",
      "40/40 [==============================] - 61s 1s/step - loss: 0.0158 - accuracy: 0.9935\n",
      "Epoch 38/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0173 - accuracy: 0.9930\n",
      "Epoch 39/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0175 - accuracy: 0.9929\n",
      "Epoch 40/70\n",
      "40/40 [==============================] - 62s 1s/step - loss: 0.0168 - accuracy: 0.9932\n",
      "Epoch 41/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0163 - accuracy: 0.9934\n",
      "Epoch 42/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0167 - accuracy: 0.9933\n",
      "Epoch 43/70\n",
      "40/40 [==============================] - 66s 2s/step - loss: 0.0168 - accuracy: 0.9933\n",
      "Epoch 44/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0146 - accuracy: 0.9939\n",
      "Epoch 45/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0138 - accuracy: 0.9944\n",
      "Epoch 46/70\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.0129 - accuracy: 0.9947\n",
      "Epoch 47/70\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.0139 - accuracy: 0.9943\n",
      "Epoch 48/70\n",
      "40/40 [==============================] - 64s 2s/step - loss: 0.0155 - accuracy: 0.9941\n",
      "Epoch 49/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0140 - accuracy: 0.9944\n",
      "Epoch 50/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0126 - accuracy: 0.9948\n",
      "Epoch 51/70\n",
      "40/40 [==============================] - 56s 1s/step - loss: 0.0129 - accuracy: 0.9949\n",
      "Epoch 52/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0155 - accuracy: 0.9940\n",
      "Epoch 53/70\n",
      "40/40 [==============================] - 57s 1s/step - loss: 0.0151 - accuracy: 0.9940\n",
      "Epoch 54/70\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.0155 - accuracy: 0.9939\n",
      "Epoch 55/70\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.0142 - accuracy: 0.9943\n",
      "Epoch 56/70\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.0161 - accuracy: 0.9938\n",
      "Epoch 57/70\n",
      "40/40 [==============================] - 57s 1s/step - loss: 0.0136 - accuracy: 0.9945\n",
      "Epoch 58/70\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.0137 - accuracy: 0.9945\n",
      "Epoch 59/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0132 - accuracy: 0.9947\n",
      "Epoch 60/70\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.0121 - accuracy: 0.9951\n",
      "Epoch 61/70\n",
      "40/40 [==============================] - 66s 2s/step - loss: 0.0141 - accuracy: 0.9947\n",
      "Epoch 62/70\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.0137 - accuracy: 0.9946\n",
      "Epoch 63/70\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.0123 - accuracy: 0.9950\n",
      "Epoch 64/70\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.0126 - accuracy: 0.9951\n",
      "Epoch 65/70\n",
      "40/40 [==============================] - 58s 1s/step - loss: 0.0118 - accuracy: 0.9953\n",
      "Epoch 66/70\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.0098 - accuracy: 0.9959\n",
      "Epoch 67/70\n",
      "40/40 [==============================] - 66s 2s/step - loss: 0.0112 - accuracy: 0.9955\n",
      "Epoch 68/70\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.0117 - accuracy: 0.9953\n",
      "Epoch 69/70\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.0117 - accuracy: 0.9955\n",
      "Epoch 70/70\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.0109 - accuracy: 0.9957\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 70\n",
    "STEPS_PER_EPOCH=40\n",
    "model_history = model.fit(data_train, labels_train, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7879d84",
   "metadata": {},
   "source": [
    "# Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "696ccb7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6cbd9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x936 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 36\n",
    "y_pred_processed = (y_pred < 0.1).astype(np.uint8)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(13,13))\n",
    "\n",
    "\n",
    "# Plotting the Ground Truth Image\n",
    "path = labels_test[idx]\n",
    "path = path == 0\n",
    "\n",
    "axs[0].imshow((data_test[idx]*255).astype(np.uint8),)\n",
    "axs[0].imshow((path*255).astype(np.uint8), cmap='Blues', alpha=0.3)\n",
    "axs[0].set_title(\"Ground Truth Image\")\n",
    "\n",
    "\n",
    "# Plotting the Predicted Image\n",
    "axs[1].imshow((data_test[idx]*255).astype(np.uint8),)\n",
    "axs[1].imshow((y_pred_processed[idx]*255).astype(np.uint8),  cmap='Blues', alpha=0.3)\n",
    "axs[1].set_title(\"Predicted Image\")\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9de17a",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
